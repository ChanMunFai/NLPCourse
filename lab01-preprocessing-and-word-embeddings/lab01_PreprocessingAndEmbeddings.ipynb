{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChanMunFai/NLPCourse/blob/main/lab01-preprocessing-and-word-embeddings/lab01_PreprocessingAndEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qS8y5_Ewv6n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Users/chanmunfai/opt/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
            "Requirement already satisfied: six in /Users/chanmunfai/opt/anaconda3/lib/python3.7/site-packages (from nltk) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE8B9-L8U0aZ"
      },
      "source": [
        "# Semantic word representations\n",
        "\n",
        "\n",
        "Recall that *semantic Word Representations* are representations that are learned to capture the 'meaning' of a word. These are low-dimensional vectors that contain some semantic properties. In this notebook we are going to build state-of-the art approaches to obtain semantic word representations using the **word2vec** modelling approach. We will also use these vectors in some  tasks to understand the utility of these representations. \n",
        "\n",
        "We begin by loading some of the libraries that are necessary for building our model. We are using [pytorch](https://pytorch.org/), an open source deep learning platform, as our backbone library in the course. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG82efAzvcbK"
      },
      "outputs": [],
      "source": [
        "#@title Loading packages\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from scipy.spatial.distance import euclidean, cosine\n",
        "from tqdm import tqdm \n",
        "import codecs\n",
        "from sklearn.metrics.pairwise import cosine_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abMSnMwJx8bu"
      },
      "outputs": [],
      "source": [
        "#@title Sample corpora\n",
        "\n",
        "corpus = [\n",
        "    'he is a king',\n",
        "    'she is a queen',\n",
        "    'he is a Man',\n",
        "    'she Is a woman',\n",
        "    'london is, the capital of England',\n",
        "    'Berlin is ... the capital of germany',\n",
        "    'paris is the capital of france.',\n",
        "    'He will eat cake, pie, and/or brownies',\n",
        "    \"she didn't like the brownies\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPlpqVFYpfL"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "Q: What is a token and why do we need to tokenize? \n",
        "\n",
        "Tokens are subunits of a text. I don't really know why we need to tokenise, but I can't imagine how to approach an NLP problem without tokenisation. I guess tokenisation is akin to splitting up text into features. \n",
        "\n",
        "Q: Print the tokenized corpus above. What mistakes do you find in the code below? \n",
        "1. ~Contains duplicates.~ This is not a mistake!  \n",
        "2. Contains punctuation -> though this is not necessarily wrong! \n",
        "3. Lower and upper words are considered different tokens. \n",
        "\n",
        "Q: What could be a nice way of fixing these mistakes? \n",
        "See code below. \n",
        "\n",
        "\n",
        "\n",
        "##### 10 mins "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ5IL1e1GVn6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'man'], ['she', 'is', 'a', 'woman'], ['london', 'is,', 'the', 'capital', 'of', 'england'], ['berlin', 'is', 'the', 'capital', 'of', 'germany'], ['paris', 'is', 'the', 'capital', 'of', 'france.'], ['he', 'will', 'eat', 'cake,', 'pie,', 'and/or', 'brownies'], ['she', \"didn't\", 'like', 'the', 'brownies']]\n"
          ]
        }
      ],
      "source": [
        "tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "for sentence in corpus:\n",
        "  tokenized_sentence = []\n",
        "  for token in sentence.split(' '): # simplest split is \n",
        "    # Add code here for Q3.\n",
        "    if any(c.isalpha() for c in token): \n",
        "      token = token.lower()\n",
        "      tokenized_sentence.append(token)\n",
        "  # tokenized_corpus.extend(tokenized_sentence)\n",
        "  tokenized_corpus.append(tokenized_sentence)\n",
        "\n",
        "\n",
        "# tokenized_corpus = list(dict.fromkeys(tokenized_corpus)) # remove duplicates\n",
        "\n",
        "print(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPWECLTEmAG9"
      },
      "source": [
        "# Pre-processing\n",
        "\n",
        "Tokenization is a crucial pre-processing step in the NLP domain`*`. However, other pre-processing techniques also exist, many of which were extensively employed in rule-based and statistical NLP. While we don't utilise these pre-processing techniques in neural-based NLP anymore, they are still worth a recap. Typically, **stop words** and **punctuation** removal are employed, along with *either* **stemming** or **lemmatization**. However, in the following code, we will demonstrate each of the techniques separately (mainly due to our corpus being so small)\n",
        "\n",
        "### Stop Word removal\n",
        "Stop words are generally the most common words in the language which who's meaning in a sequenece is ambiguous. Some examples of stop words are: The, a, an, that.\n",
        "\n",
        "### Punctuation removal\n",
        "Old school NLP techniques (and some modern day ones) struggle to understand the semantics of punctuation. Thus, they were also removed.\n",
        "\n",
        "## Stemming and Lemmatization\n",
        "Stemming and Lemmatization are two distinct word normalization techniques. Essentially this means that, given our corpora, we wish to have variants of a word in a 'normal' form. For example, [playing, plays, played] may be normalised to \"Play\". The sentence \"the boy's cars are different colours\" may be normalised to \"the boy car be differ colour\"\n",
        "\n",
        "### Stemming\n",
        "In the case of stemming, we want to normalise all words to their stem (or root). The stem is the part of the word to which affixes (suffixes or prefixes) are assigned. Stemming a word may result in the word not actually being a word. For example, some stemming algorithms may stem [trouble, troubling, troubled] as \"troubl\".\n",
        "\n",
        "### Lemmatization\n",
        "Lemmatization attempts to properly reduce unnormalized tokens to a word that belongs in the language. The root word is called a **lemma**, and is the canonical form of a set of words. For example, [runs, running, ran] are all forms of the word \"run.\n",
        "\n",
        "\n",
        "\n",
        "Q. Think of two or three other stop words, and add them to the list of stop words below.\n",
        "\n",
        "Done\n",
        "\n",
        "\n",
        "Q. Write some code which both removes stop words and punctuation from our corpus\n",
        "\n",
        "Done\n",
        "\n",
        "\n",
        "Q. The examples of stemming and lemmatization below are on words/sequences not in our corpus. Extend the code so it works on our corpus.\n",
        "\n",
        "##### 10 mins \n",
        "\n",
        "N.B. We are not going to use these techniques in this file after this section, so we will demonstrate how to perform these techniques distinctly on our toy corpus.\n",
        "\n",
        "`*`Recently there has been newer approaches to \"tokenization\" which goes further than one token being one word. One example is [SentencePiece](https://github.com/google/sentencepiece). These approaches are out of scope for this lab session, but may appear in future sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_DGoHbPmAG-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/chanmunfai/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/chanmunfai/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # Download the tokenizer model\n",
        "nltk.download('wordnet') # Download the wordnet corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuSjoUpfmAHA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['he', 'king'], ['she', 'queen'], ['he', 'Man'], ['she', 'Is', 'woman'], ['london', 'is,', 'capital', 'of', 'England'], ['Berlin', '...', 'capital', 'of', 'germany'], ['paris', 'capital', 'of', 'france.'], ['He', 'eat', 'cake,', 'pie,', 'and/or', 'brownies'], ['she', \"didn't\", 'like', 'brownies']]\n"
          ]
        }
      ],
      "source": [
        "# STOP WORD REMOVAL\n",
        "stop_words_list = [\"the\", \"a\", \"an\", \"that\", \"is\", \"will\"] # Add stop words from Q1 here.\n",
        "\n",
        "# SWR = stop words removed\n",
        "tokenized_corpus_SWR = []\n",
        "for sentence in corpus:\n",
        "    tokenized_sentence_SWR = []\n",
        "    \n",
        "    for token in sentence.split(\" \"):\n",
        "        if token not in stop_words_list:\n",
        "            tokenized_sentence_SWR.append(token)\n",
        "\n",
        "    if tokenized_sentence_SWR: # Only append to corpus if tokenized_sentence_SWR isn't empty\n",
        "        tokenized_corpus_SWR.append(tokenized_sentence_SWR)\n",
        "        \n",
        "print(tokenized_corpus_SWR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VtK8AQ_mAHC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'Man'], ['she', 'Is', 'a', 'woman'], ['london', 'is', 'the', 'capital', 'of', 'England'], ['Berlin', 'is', 'the', 'capital', 'of', 'germany'], ['paris', 'is', 'the', 'capital', 'of', 'france'], ['He', 'will', 'eat', 'cake', 'pie', 'and', 'or', 'brownies'], ['she', 'didn', 't', 'like', 'the', 'brownies']]\n"
          ]
        }
      ],
      "source": [
        "# PUNCTUATION REMOVAL\n",
        "import re # regex\n",
        "\n",
        "re_punctuation_string = '[\\s,/.\\']'\n",
        "\n",
        "# PR = punctuation removed\n",
        "tokenized_corpus_PR = []\n",
        "for sentence in corpus:\n",
        "    tokenized_sentence_PR = re.split(re_punctuation_string, sentence) # in python's regex, [...] is an alternative to writing .|.|.\n",
        "    tokenized_sentence_PR = list(filter(None, tokenized_sentence_PR)) # remove empty strings from list \n",
        "    tokenized_corpus_PR.append(tokenized_sentence_PR)\n",
        "        \n",
        "print(tokenized_corpus_PR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyzTW_wXmAHF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['he', 'king', 'she', 'queen', 'he', 'Man', 'she', 'Is', 'woman', 'london', 'capital', 'of', 'England', 'Berlin', 'capital', 'of', 'germany', 'paris', 'capital', 'of', 'france', 'He', 'eat', 'cake', 'pie', 'and', 'or', 'brownies', 'she', 'didn', 't', 'like', 'brownies']\n"
          ]
        }
      ],
      "source": [
        "# ANSWER Q2 HERE\n",
        "\n",
        "tokenized_corpus_PR_SWR = []\n",
        "tokenized_sentence_PR_SWR = []\n",
        "\n",
        "for sentence in corpus:\n",
        "    tokenized_sentence_PR = re.split(re_punctuation_string, sentence) # in python's regex, [...] is an alternative to writing .|.|.\n",
        "    tokenized_sentence_PR = list(filter(None, tokenized_sentence_PR)) # remove empty strings from list \n",
        "\n",
        "    for token in tokenized_sentence_PR:\n",
        "        if token not in stop_words_list:\n",
        "            tokenized_corpus_PR_SWR.append(token)\n",
        "        \n",
        "print(tokenized_corpus_PR_SWR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZGX31WamAHH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word                Stemmed variant     \n",
            "\n",
            "friend              friend              \n",
            "friendship          friendship          \n",
            "friends             friend              \n",
            "friendships         friendship          \n",
            "stabil              stabil              \n",
            "destabilize         destabil            \n",
            "misunderstanding    misunderstand       \n",
            "railroad            railroad            \n",
            "moonlight           moonlight           \n",
            "football            footbal             \n"
          ]
        }
      ],
      "source": [
        "# STEMMING\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "stemming_word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Stemmed variant\"))\n",
        "print()\n",
        "\n",
        "for word in stemming_word_list:\n",
        "      print(\"{0:20}{1:20}\".format(word,porter.stem(word)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Muvs2GZ4mAHK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word                Lemma               \n",
            "\n",
            "He                  He                  \n",
            "was                 wa                  \n",
            "running             running             \n",
            "and                 and                 \n",
            "eating              eating              \n",
            "at                  at                  \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 ha                  \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swimming            \n",
            "after               after               \n",
            "playing             playing             \n",
            "long                long                \n",
            "hours               hour                \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "Sun                 Sun                 \n"
          ]
        }
      ],
      "source": [
        "# LEMMATIZATION\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "to_lemmatize_sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "\n",
        "# lemmatization requires punctuation removal\n",
        "to_lemmatize_sentence = re.split(re_punctuation_string, to_lemmatize_sentence)\n",
        "to_lemmatize_sentence = list(filter(None, to_lemmatize_sentence))\n",
        "\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "print()\n",
        "\n",
        "for word in to_lemmatize_sentence:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL6qkUJPmAHM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word                Lemma               \n",
            "\n",
            "He                  He                  \n",
            "was                 be                  \n",
            "running             run                 \n",
            "and                 and                 \n",
            "eating              eat                 \n",
            "at                  at                  \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 have                \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swim                \n",
            "after               after               \n",
            "playing             play                \n",
            "long                long                \n",
            "hours               hours               \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "Sun                 Sun                 \n"
          ]
        }
      ],
      "source": [
        "# Why didn't the above do anything?\n",
        "# It's because the lemmatizer requires parts of speech (POS) context about the word it is currently parsing.\n",
        "# We would need to use a POS model to identify what the POS for a token in its context is.\n",
        "# In the above example (and for Q3), we'll just pass in the VERB context for every token\n",
        "\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "print()\n",
        "\n",
        "for word in to_lemmatize_sentence:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXfoaPJ3mAHP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word                Lemma               \n",
            "\n",
            "he                  he                  \n",
            "is                  be                  \n",
            "a                   a                   \n",
            "king                king                \n",
            "she                 she                 \n",
            "queen               queen               \n",
            "Man                 Man                 \n",
            "Is                  Is                  \n",
            "woman               woman               \n",
            "london              london              \n",
            "the                 the                 \n",
            "capital             capital             \n",
            "of                  of                  \n",
            "England             England             \n",
            "Berlin              Berlin              \n",
            "germany             germany             \n",
            "paris               paris               \n",
            "france              france              \n",
            "He                  He                  \n",
            "will                will                \n",
            "eat                 eat                 \n",
            "cake                cake                \n",
            "pie                 pie                 \n",
            "and                 and                 \n",
            "or                  or                  \n",
            "brownies            brownies            \n",
            "didn                didn                \n",
            "t                   t                   \n",
            "like                like                \n"
          ]
        }
      ],
      "source": [
        "# ANSWER Q3 HERE (make sure not to overwrite our \"tokenized_corpus\" variable from the \"\")\n",
        "\n",
        "to_lemmatize_sentence = \" \".join(corpus)\n",
        "\n",
        "# lemmatization requires punctuation removal\n",
        "to_lemmatize_sentence = re.split(re_punctuation_string, to_lemmatize_sentence)\n",
        "to_lemmatize_sentence = list(filter(None, to_lemmatize_sentence))\n",
        "to_lemmatize_sentence = list(dict.fromkeys(to_lemmatize_sentence))\n",
        "# print(to_lemmatize_sentence)\n",
        "\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "print()\n",
        "\n",
        "for word in to_lemmatize_sentence:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos = \"v\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m_RNjN-_K4E"
      },
      "source": [
        "Note that other NLP tools, such as [SpaCy](https://spacy.io/), or [Stanza](https://stanfordnlp.github.io/stanza/) are popular alternatives which provide higher levels of abstractions than NLTK. When working on an NLP task, the use of one of those two libraries is recommended over NLTK.\n",
        "\n",
        "The code below will run through implementing a Word2Vec algorithm from scratch. A fuller and wholesome tutorial can be found in the \"DeeperDiveIntoWordEmbeddings.zip\" folder. Feel free to take a read through the notebook there if you want more information, or if you found yourself struggling to understand every concept from the taught theory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyKCfY6QbO0r"
      },
      "source": [
        "# Vocabulary\n",
        "\n",
        "The code below obtains the vocabulary of the corpus. \n",
        "\n",
        "Q. Print the size of the vocabulary.\n",
        "\n",
        "Q. A programatically cleaner (and shorter) way of writing the code below by using a set instead of a list. Can you implement the code below using a set?\n",
        "\n",
        "##### 3 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bWQ3zKYKlQU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['he', 'is', 'a', 'king', 'she', 'queen', 'man', 'woman', 'london', 'is,', 'the', 'capital', 'of', 'england', 'berlin', 'germany', 'paris', 'france.', 'will', 'eat', 'cake,', 'pie,', 'and/or', 'brownies', \"didn't\", 'like']\n",
            "26\n"
          ]
        }
      ],
      "source": [
        "vocabulary = [] # Let us put all the tokens (mostly words) \n",
        "                # appearing in the vocabulary in a list\n",
        "  \n",
        "for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "\n",
        "\n",
        "print(vocabulary)\n",
        "\n",
        "\n",
        "\n",
        "# Q. what is the size of the vocabulary?\n",
        "\n",
        "# A. uncomment and fill below - 26\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "print(vocabulary_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n"
          ]
        }
      ],
      "source": [
        "vocabulary = set()\n",
        "for sentence in tokenized_corpus: \n",
        "    for token in sentence: \n",
        "        vocabulary.add(token)\n",
        "\n",
        "print(len(vocabulary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGg6CtALe8Va"
      },
      "source": [
        "# Helper functions \n",
        "\n",
        "* These are some of the common helper functions that are used for NLP models:\n",
        "\n",
        "    * `word2idx`:  Maintains a dictionary of word and the corresponding index\n",
        "    \n",
        "    * `idx2word`: Maintains a mapping from index to word \n",
        "    \n",
        "    \n",
        "* Print the word2idx and idx2word, we will be using these in future exercises. \n",
        "\n",
        "##### 3 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LWut1gtXGQN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eat': 0, 'brownies': 1, 'like': 2, 'will': 3, 'is': 4, 'she': 5, 'the': 6, 'berlin': 7, 'london': 8, 'cake,': 9, 'capital': 10, 'he': 11, 'king': 12, 'pie,': 13, \"didn't\": 14, 'paris': 15, 'and/or': 16, 'is,': 17, 'woman': 18, 'man': 19, 'queen': 20, 'a': 21, 'england': 22, 'germany': 23, 'france.': 24, 'of': 25}\n",
            "{0: 'eat', 1: 'brownies', 2: 'like', 3: 'will', 4: 'is', 5: 'she', 6: 'the', 7: 'berlin', 8: 'london', 9: 'cake,', 10: 'capital', 11: 'he', 12: 'king', 13: 'pie,', 14: \"didn't\", 15: 'paris', 16: 'and/or', 17: 'is,', 18: 'woman', 19: 'man', 20: 'queen', 21: 'a', 22: 'england', 23: 'germany', 24: 'france.', 25: 'of'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "print(word2idx)\n",
        "print(idx2word)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bFjK86NPChI"
      },
      "source": [
        "# Look-up table \n",
        "\n",
        "* This is a table that maps from an index to a one hot vector. \n",
        "\n",
        "Q. Print one-hot vectors corresponding to the words 'this', 'he' and ''england'\n",
        "\n",
        "##### 3 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp8OTZI-UrYU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Word does not exist\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def look_up_table(word_idx):\n",
        "    x = torch.zeros(vocabulary_size).float()\n",
        "    x[word_idx] = 1.0\n",
        "    return x\n",
        "  \n",
        "# This is a one hot representation\n",
        "\n",
        "# Q. try printing it for word_idx = 1\n",
        "print(look_up_table(1))\n",
        "\n",
        "word_idx = word2idx['he']\n",
        "print(look_up_table(word_idx))\n",
        "\n",
        "try: \n",
        "    word_idx = word2idx['this']\n",
        "    print(look_up_table(word_idx))\n",
        "except: \n",
        "    print(f\"Word does not exist\")\n",
        "\n",
        "word_idx = word2idx['england']\n",
        "print(look_up_table(word_idx))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcAI5BtaQMFh"
      },
      "source": [
        "# Extracting contexts and the focus word\n",
        "\n",
        "\n",
        "Recall that we are building the skip-gram model. \n",
        "\n",
        "**We first begin by obtaining the set of contexts and focus words.**\n",
        "* Let's say we have a sentence (represented as vocabulary indicies): `[0, 2, 3, 6, 7]`.\n",
        "* For every word in the sentence, we want to get the words which are `window_size` around it.\n",
        "* So if `window_size==2`, for the word '0', we obtain: `[[0, 2], [0, 3]]`\n",
        "* For the word '2', we obtain: `[[2, 0], [2, 3], [2, 6]]`\n",
        "* For the word '3', we obtain: `[[3, 0], [3, 2], [3, 6], [3, 7]]`\n",
        "\n",
        "Q. Print some of the index pairs and trace them back to their words. \n",
        "\n",
        "\n",
        "##### 10 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrtOwTUsyArb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "he is\n",
            "he a\n",
            "is he\n",
            "is a\n",
            "is king\n"
          ]
        }
      ],
      "source": [
        "window_size = 2\n",
        "\n",
        "idx_pairs = []\n",
        "\n",
        "# variables of interest: \n",
        "#   center_word_pos: center word position\n",
        "#   context_word_pos: context_word_position\n",
        "#   add sentence length as a constraint\n",
        "\n",
        "for sentence in tokenized_corpus:\n",
        "    indices = [word2idx[word] for word in sentence]\n",
        "    \n",
        "    for center_word_pos in range(len(indices)):\n",
        "        \n",
        "        for w in range(-window_size, window_size + 1):\n",
        "            context_word_pos = center_word_pos + w\n",
        "            \n",
        "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                continue\n",
        "                \n",
        "            context_word_idx = indices[context_word_pos]\n",
        "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
        "\n",
        "# print(idx_pairs)\n",
        "\n",
        "selected_idx_pairs = idx_pairs[:5]\n",
        "# print(selected_idx_pairs)\n",
        "\n",
        "for pair in selected_idx_pairs: \n",
        "    print(idx2word[pair[0]],idx2word[pair[1]] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLzHkq6FRULl"
      },
      "source": [
        "# Parameters and hyperparameters \n",
        "\n",
        "* For our toy task, let us set the embedding dimensions to 5\n",
        "* Let us run the algorithm for 10 epochs (number of times the training algorithm looks at the corpus/training data)\n",
        "* Let us choose the learning rate as 0.001\n",
        "\n",
        "We have two parameter matrices $W_1$ and $W_2$ - the embedding matrix and the weight matrix. \n",
        "\n",
        "Q. What are the dimensionalities of $W_1$ and $W_2$?\n",
        "\n",
        "The dimension of the embedding matrix is |vocabulary| X 5, as it maps |vocabulary| inputs to 5 outputs. \n",
        "The dimension of the context matrix is 5 X |vocabularly|. \n",
        "\n",
        "* Why is this different here?\n",
        "It is different because in the implementation, they multiply the weight before the feature. My answer is still correct.\n",
        "\n",
        "\n",
        "##### 3 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kBhE6FeRuDu"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters:\n",
        "embedding_dims = 5\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# The two weight matrices:\n",
        "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
        "W2 = torch.randn(vocabulary_size, embedding_dims, requires_grad=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzWXeQATPs4"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "(Refer to Lecture 2 slides 30-31)\n",
        "\n",
        "In the code below, we are going to compute the log probability of the correct context (target) given the word. \n",
        "\n",
        "Before running the code, answer the question commented in the code -> fill `y_true`.\n",
        "\n",
        "Print the loss and see if the loss goes down.\n",
        "\n",
        "###### 10 mins\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "5Cx2eJi1a8R6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:03<00:00, 26.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final epoch loss: 3.3253594691936788\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  \n",
        "    loss_val = 0\n",
        "    \n",
        "    for data, target in idx_pairs: # data is current word, target is context word \n",
        "      \n",
        "        x = torch.Tensor(look_up_table(data)) #, requires_grad=True) # x is a One-hot tensor\n",
        "\n",
        "        # Q. what would y_true be? \n",
        "        y_true = torch.Tensor([target]).long()\n",
        "\n",
        "        # A. \n",
        "        # This is wrong and seems to be unnecessary \n",
        "        # y_true = look_up_table(y_true).view(1, -1)\n",
        "        # print(y_true.shape)\n",
        "\n",
        "        # \n",
        "        z1 = torch.matmul(W1, x) \n",
        "        # Q. what is z1? # new vector representation of x\n",
        "        \n",
        "        z2 = torch.matmul(W2, z1)\n",
        "        # print(z2.shape) # 26 \n",
        "        # Q. what is the above operation? \n",
        "    \n",
        "        # Let us obtain prediction over the vocabulary\n",
        "        log_softmax = F.log_softmax(z2, dim=0)\n",
        "        \n",
        "        # Our loss is a negative log-likelihood loss \n",
        "        # (what does this mean?)\n",
        "        \n",
        "        # print(log_softmax.view(1,-1))\n",
        "        # print(log_softmax.view(1,-1).shape)\n",
        "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
        "        \n",
        "        loss_val += loss.item()\n",
        "        \n",
        "        # propagate the error\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        # zero out gradient accumulation\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "\n",
        "    # print(loss_val/len(idx_pairs))\n",
        "\n",
        "print(f'\\nFinal epoch loss: {loss_val/len(idx_pairs)}')        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPsXq60HUWWq"
      },
      "source": [
        "Q. Given that we are interested in distributed representations, what is the major bottleneck in our setup? Is it the dimensionality of the representations? Is it the learning rate? Is it the corpus? \n",
        "\n",
        "Major bottleneck is the size of the weights (i.e. number of parameters). If we have more words in our corpus, then the matrix is significantly bigger. \n",
        "\n",
        "Q. What hyperparameters would you tune to improve the representations? \n",
        "\n",
        "Q. Train the algorithm with a bigger corpus. \n",
        "\n",
        "(You can either copy and paste the corpus and bring it to the same format as the corpus above or use the hint below)\n",
        "\n",
        "###### 10 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "ixhvaYkW_SfX"
      },
      "outputs": [],
      "source": [
        "# Example code for getting corpora from the internet\n",
        "import urllib\n",
        "txt = [line.strip() for line in urllib.request.urlopen('https://raw.githubusercontent.com/luonglearnstocode/Seinfeld-text-corpus/master/corpus.txt').readlines()]\n",
        "txt = [i.decode(\"utf-8\") for i in txt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JERRY: Do you know what this is all about? Do you know, why we're here? To be out, this is out...and out is one of the single most enjoyable experiences of life. People...did you ever hear people talking about \"We should go out\"? This is what they're talking about...this whole thing, we're all out now, no one is home. Not one person here is home, we're all out! There are people tryin' to find us, they don't know where we are. (on an imaginary phone) \"Did you ring?, I can't find him.\" \"Where did he go?\" \"He didn't tell me where he was going\". He must have gone out. You wanna go out: you get ready, you pick out the clothes, right? You take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...Then you're standing around, whatta you do? You go: \"We gotta be getting back\". Once you're out, you wanna get back! You wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? Where ever you are in life, it's my feeling, you've gotta go.\n"
          ]
        }
      ],
      "source": [
        "print(txt[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenise text with punctuation removal and stop word removal\n",
        "\n",
        "tokenized_corpus_PR_SWR = []\n",
        "tokenized_sentence_PR_SWR = []\n",
        "\n",
        "\n",
        "for sentence in txt:\n",
        "    tokenized_sentence_PR = re.split(re_punctuation_string, sentence) # in python's regex, [...] is an alternative to writing .|.|.\n",
        "    tokenized_sentence_PR = list(filter(None, tokenized_sentence_PR)) # remove empty strings from list \n",
        "\n",
        "    for token in tokenized_sentence_PR:\n",
        "        if token not in stop_words_list:\n",
        "            tokenized_corpus_PR_SWR.append(token)\n",
        "        \n",
        "# print(tokenized_corpus_PR_SWR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n"
          ]
        }
      ],
      "source": [
        "vocabulary = set()\n",
        "for sentence in tokenized_corpus: \n",
        "    for token in sentence: \n",
        "        vocabulary.add(token)\n",
        "\n",
        "print(len(vocabulary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2znwTJeU3UT"
      },
      "source": [
        "# Using word embeddings\n",
        "\n",
        "One of the simplest ways of exploiting word representations is to find similar words. There are many ways of measuring the semantic similarity between two words. As we are using word representations which are vectors in the euclidean space, distance metrics defined in the euclidean space are the most popular choice. This is because words that share common contexts in the corpus are located in close proximity to one another in the euclidean space.  One such metric is the eucldeian distance.\n",
        "\n",
        "Q. What is the euclidean distance between 'the' and 'a' (in the sample corpus and the new corpus)? \n",
        "1.7463502883911133 in the sample corpus. \n",
        "\n",
        "Q. What other distance metrics can we use for two vectors? \n",
        "\n",
        "Cos similarity that does normalises for the length of the vectors (frequency)\n",
        "\n",
        "\n",
        "\n",
        "###### 10 mins\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "NUPETZaOgvgB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.9633183479309082\n",
            "3.240849733352661\n",
            "1.6134350895881653\n"
          ]
        }
      ],
      "source": [
        "from scipy import spatial\n",
        "\n",
        "# Let us get two vectors from the trained model\n",
        "\n",
        "x = torch.Tensor(look_up_table(0))\n",
        "x_emb = torch.matmul(W1, x).detach().numpy()\n",
        "y = torch.Tensor(look_up_table(1))\n",
        "y_emb = torch.matmul(W1, y).detach().numpy()\n",
        "\n",
        "# let us print the euclidean distance\n",
        "print(euclidean(x_emb, y_emb))\n",
        "\n",
        "idx_the = word2idx[\"the\"]\n",
        "idx_the = torch.Tensor(look_up_table(idx_the))\n",
        "idx_a = word2idx[\"a\"]\n",
        "idx_a = torch.Tensor(look_up_table(idx_a))\n",
        "\n",
        "the_emb = torch.matmul(W1, idx_the).detach().numpy()\n",
        "a_emb = torch.matmul(W1, idx_a).detach().numpy()\n",
        "print(euclidean(the_emb, a_emb))\n",
        "\n",
        "print(spatial.distance.cosine(the_emb, a_emb)) # cosine distance "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UZroMvLtFVv"
      },
      "source": [
        "# ADVANCED: Training with negative sampling \n",
        " \n",
        " \n",
        " \n",
        "\n",
        "Refer to skipgram models in the slides. \n",
        "\n",
        "Q. What happens when we have a very large vocabulary? \n",
        "\n",
        "Size of matrices increase. \n",
        "\n",
        "Q. What is a negative sample? \n",
        "\n",
        "It is a sample for which the desired output is 0, i.e. it does not appear in the context window of the target word. \n",
        "\n",
        "\n",
        "Below is the code for training the model with negative sampling. \n",
        "\n",
        "\n",
        "##### 10 mins \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "Lq1lcWbqRwNY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss at epo 0: 2.7675078245023124\n",
            "Loss at epo 10: 2.451729760433619\n",
            "Loss at epo 20: 2.485786211118102\n",
            "Loss at epo 30: 2.345728284235184\n",
            "Loss at epo 40: 2.219196591913127\n",
            "Loss at epo 50: 2.121277701224272\n",
            "Loss at epo 60: 2.049541345381966\n",
            "Loss at epo 70: 2.0268701075074764\n",
            "Loss at epo 80: 1.9597840020862909\n",
            "Loss at epo 90: 1.8807630566593545\n"
          ]
        }
      ],
      "source": [
        "# The two weight matrices:\n",
        "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
        "W2 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for data, target in idx_pairs:\n",
        "        x_var = Variable(look_up_table(data)).float() \n",
        "        \n",
        "        y_pos = Variable(torch.from_numpy(np.array([target])).long())\n",
        "        y_pos_var = Variable(look_up_table(target)).float()\n",
        "        \n",
        "        neg_sample = np.random.choice(list(range(vocabulary_size)),size=(1))[0] # How do I ensure that neg is not y_pos?\n",
        "\n",
        "        y_neg = Variable(torch.from_numpy(np.array([neg_sample])))\n",
        "        y_neg_var = Variable(look_up_table(neg_sample)).float()\n",
        "         \n",
        "        x_emb = torch.matmul(W1, x_var) \n",
        "        y_pos_emb = torch.matmul(W2, y_pos_var)\n",
        "        y_neg_emb = torch.matmul(W2, y_neg_var)\n",
        "        \n",
        "        # get positive sample score\n",
        "        pos_loss = F.logsigmoid(torch.matmul(x_emb, y_pos_emb)) # sigmoid loss on positive sample\n",
        "        \n",
        "        # get negsample score\n",
        "        neg_loss = F.logsigmoid(-1 * torch.matmul(x_emb, y_neg_emb)) # sigmoid loss on negative example\n",
        "        \n",
        "        loss = - (pos_loss + neg_loss)\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        # propagate the error\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        # zero out gradient accumulation\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "        \n",
        "    if epoch % 10 == 0:    \n",
        "        print(f'Loss at epo {epoch}: {epoch_loss/len(idx_pairs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGgcB8VHXvPK"
      },
      "source": [
        "* In the current setup, we are only exploiting a very small sample of negative examples. This is suboptimal. \n",
        "\n",
        "* Given a sufficiently large vocabulary, we would ideally sample the negative samples from a noise distribution whose probabilities match the frequency of vocabulary.\n",
        "\n",
        "\n",
        "Q.  Using this code as the basis, build an object oriented negative sampling based model and train it on the fairly large corpus. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "Y_Eknuozunq5"
      },
      "outputs": [],
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4VCX4XSqvlu"
      },
      "source": [
        "# Pre-trained representations\n",
        "\n",
        "We have seen from the above that word embeddings are learned in an unsupervised manner, i.e., we don't have any labelled data. These representations can be used to `bootstrap' models in NLP. There are many word representation inducing algorithms : [word2vec](https://arxiv.org/abs/1301.3781), [GloVe](https://nlp.stanford.edu/pubs/glove.pdf), [Fasttext](https://arxiv.org/abs/1607.04606) are some of the popular choices. There are differences in the algorithms but they are all based on the distributional hypothesis. \n",
        "\n",
        "We will now use one of these pre-trained representations: GloVe. \n",
        "\n",
        "Q. What is the dimensionality of the representations below? \n",
        "50 \n",
        "##### 2 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCsV8mtBg4-Y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400000/400000 [00:12<00:00, 30971.40it/s]\n"
          ]
        }
      ],
      "source": [
        "w2i = [] # word2index\n",
        "i2w = [] # index2word\n",
        "wvecs = [] # word vectors\n",
        "\n",
        "# this is a large file, it will take a while to load in the memory!\n",
        "with codecs.open('glove.6B.50d.txt', 'r','utf-8') as f: \n",
        "  index = 0\n",
        "  for line in tqdm(f.readlines()):\n",
        "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
        "    if len(line.strip().split()) > 3:\n",
        "      \n",
        "      (word, vec) = (line.strip().split()[0], \n",
        "                     list(map(float,line.strip().split()[1:]))) \n",
        "      \n",
        "      wvecs.append(vec)\n",
        "      w2i.append((word, index))\n",
        "      i2w.append((index, word))\n",
        "      index += 1\n",
        "\n",
        "w2i = dict(w2i)\n",
        "i2w = dict(i2w)\n",
        "wvecs = np.array(wvecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "400000\n",
            "4298\n",
            "kernels\n",
            "[-0.0053221 -1.2119     0.23615    0.66215    0.097367   0.09544\n",
            " -0.046128  -0.79512   -0.17979    0.34579    0.14464    0.16585\n",
            "  1.0863    -0.097819   0.65772    0.36766   -0.33688    0.089284\n",
            "  0.26065   -0.93269   -0.87663   -1.5887     1.5867    -0.021235\n",
            "  0.081774   1.1644     0.5547     0.76308    0.7955    -0.30241\n",
            "  0.72303   -0.69054   -0.77696    1.4036    -0.31992    0.22547\n",
            " -0.80432    0.44514    0.73697   -0.26593    0.36781   -0.34146\n",
            " -1.385      1.2478     0.066696   1.1887     1.0335     0.12906\n",
            " -0.99628   -1.1382   ]\n",
            "(50,)\n"
          ]
        }
      ],
      "source": [
        "print(len(wvecs))\n",
        "print(w2i[\"chicken\"])\n",
        "print(i2w[37751])\n",
        "print(wvecs[37751])\n",
        "print(wvecs[37751].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cP-UmR5Br4_"
      },
      "source": [
        "For the following experiments, we recommend  using `wvecs` - the pretrained representations. \n",
        "\n",
        "# Evaluating word representation models\n",
        "\n",
        "## Inrtinsic Evaluation\n",
        "\n",
        "* Intrinsic evaluation of word representations involves evaluating  set of word vectors generated by an embedding technique on specific  subtasks that in someways are directly related to the distributional hypothesis. These are typically simple and fast to compute and thereby allow us to help understand representation learning algorithms.\n",
        "\n",
        "* An intrinsic evaluation should typically return to us a scalar quantity that measures the performance of those word vectors on the evaluation subtask.\n",
        "\n",
        "\n",
        "\n",
        "## Word Similarity\n",
        "\n",
        "The first task we consider is evaluating if the representations are good at computing if two words are similar. In this task, you will use both euclidean distance or cosine distance as similarity measures. \n",
        "\n",
        "* Print similarity scores for word pairs in https://github.com/iraleviant/eval-multilingual-simlex/blob/master/evaluation/ws-353/wordsim353-english-sim.txt\n",
        "\n",
        "     (Format of the file: two words and the corresponding human score for the two words)\n",
        "\n",
        "* Obtain pearson's correlation with predicted scores and the human generated scores. \n",
        "(0.4246259666521494, 6.801092849706633e-10) where the first value is the correlation and 2nd value the p value. \n",
        "\n",
        "##### 15 mins\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_distance(u, v):\n",
        "    distance = 0.0\n",
        "    dot = np.dot(u,v)\n",
        "    norm_u = np.sqrt(np.sum(u**2))\n",
        "    norm_v = np.sqrt(np.sum(v**2))\n",
        "    distance = dot/(norm_u)/norm_v\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tiger cat 0.8310000000000001 0.6150732418405161\n",
            "tiger tiger 1.0 1.0\n",
            "plane car 0.585 0.6656114206912427\n",
            "train car 0.631 0.7658227528614754\n",
            "television radio 0.6849999999999999 0.8709275278715246\n",
            "media radio 0.808 0.7614966025988954\n",
            "bread butter 0.754 0.8402199989344501\n",
            "cucumber potato 0.592 0.7118516503650522\n",
            "doctor nurse 0.8150000000000001 0.7977497347874546\n",
            "professor doctor 0.5309999999999999 0.5824731746059034\n",
            "student professor 0.654 0.672044328547566\n",
            "smart student 0.658 0.40652697584786424\n",
            "smart stupid 0.592 0.6181943426538674\n",
            "stock phone 0.154 0.5236597198261208\n",
            "stock jaguar 0.154 0.207980845411382\n",
            "stock egg 0.162 0.22105772094366385\n",
            "stock live 0.369 0.2711624205965431\n",
            "stock life 0.185 0.38823228726019743\n",
            "wood forest 0.8380000000000001 0.5854366178142411\n",
            "money cash 0.9380000000000001 0.8989869711257938\n",
            "professor cucumber 0.023 -0.130357802619789\n",
            "king cabbage 0.11499999999999999 0.11323214733125984\n",
            "king queen 0.8460000000000001 0.7839043010964117\n",
            "king rook 0.592 0.2544332119182013\n",
            "bishop rabbi 0.8230000000000001 0.5044326797121506\n",
            "holy sex 0.146 0.33792383513469854\n",
            "fuck sex 0.931 0.16514692457540922\n",
            "football basketball 0.708 0.8786788106585918\n",
            "football tennis 0.646 0.5968911925563081\n",
            "physics chemistry 0.7769999999999999 0.8995856999334901\n",
            "space chemistry 0.377 0.3329512596028915\n",
            "vodka gin 0.8150000000000001 0.6609975207208769\n",
            "vodka brandy 0.7849999999999999 0.7155877780005409\n",
            "drink car 0.077 0.43684305276304836\n",
            "drink ear 0.038 0.35210192355721287\n",
            "drink eat 0.661 0.7511290388311628\n",
            "drink mother 0.131 0.4102547406272907\n",
            "car automobile 0.954 0.6956217460301118\n",
            "gem jewel 0.931 0.7385609020096757\n",
            "journey voyage 0.954 0.8270825728082223\n",
            "boy lad 0.9460000000000001 0.42798375918647996\n",
            "asylum madhouse 0.9 -0.2736950309287808\n",
            "magician wizard 0.877 0.7236876342931902\n",
            "furnace stove 0.769 0.6653074552749286\n",
            "food fruit 0.8150000000000001 0.6762282741315784\n",
            "bird cock 0.6849999999999999 0.2324910936681192\n",
            "bird crane 0.8310000000000001 0.44927158105578324\n",
            "crane implement 0.308 -0.08689874632600733\n",
            "lad brother 0.562 0.2478366014643359\n",
            "monk oracle 0.43099999999999994 0.05057901329127991\n",
            "cemetery woodland 0.215 0.5166714349355958\n",
            "food rooster 0.292 -0.02467998639418979\n",
            "coast hill 0.315 0.458768818824192\n",
            "forest graveyard 0.2 0.33970089164425804\n",
            "shore woodland 0.3 0.40901974627379545\n",
            "monk slave 0.215 0.4327132641654436\n",
            "coast forest 0.292 0.6268686508859369\n",
            "lad wizard 0.185 0.3729129076848966\n",
            "chord smile 0.123 0.2416672068340998\n",
            "glass magician 0.11499999999999999 0.1535895213153336\n",
            "noon string 0.046 0.21977023748342966\n",
            "rooster voyage 0.054000000000000006 0.010636542049305127\n",
            "money dollar 0.908 0.5996527813053378\n",
            "money cash 0.9380000000000001 0.8989869711257938\n",
            "money currency 0.954 0.5338524094905931\n",
            "money operation 0.3 0.5110117944343125\n",
            "tiger jaguar 0.842 0.40385548767064805\n",
            "tiger feline 0.8460000000000001 0.30334555273356767\n",
            "tiger carnivore 0.892 0.18057826872156912\n",
            "tiger mammal 0.881 0.3407802316339837\n",
            "tiger animal 0.906 0.453913874212602\n",
            "tiger organism 0.6769999999999999 0.07573844965458655\n",
            "tiger fauna 0.43099999999999994 0.2693664751100741\n",
            "psychology psychiatry 0.835 0.8043317622564078\n",
            "psychology science 0.6809999999999999 0.8286370983769055\n",
            "psychology discipline 0.623 0.5264090031445179\n",
            "planet star 0.808 0.4764140708384465\n",
            "planet moon 0.8150000000000001 0.652426208993239\n",
            "planet sun 0.858 0.520300511540505\n",
            "precedent example 0.6849999999999999 0.5176733483098173\n",
            "precedent information 0.554 0.3390122021693021\n",
            "precedent cognition 0.41500000000000004 0.19397604892105008\n",
            "precedent collection 0.392 0.21338902771180285\n",
            "precedent group 0.238 0.19253018476257114\n",
            "precedent antecedent 0.669 0.3452852525985426\n",
            "cup tableware 0.754 0.09758225279356385\n",
            "cup article 0.185 0.1717731088828934\n",
            "cup artifact 0.354 -0.04619209943837343\n",
            "cup object 0.662 0.05836221641422629\n",
            "cup entity 0.20800000000000002 0.05744854403519164\n",
            "cup food 0.41500000000000004 0.3181414970690662\n",
            "cup substance 0.369 0.25963247708064624\n",
            "jaguar cat 0.885 0.3728370879381851\n",
            "jaguar car 0.735 0.47451340632665395\n",
            "energy secretary 0.092 0.39219260568587644\n",
            "investigation effort 0.55 0.5756133090949908\n",
            "image surface 0.404 0.4793906769388473\n",
            "sign recess 0.142 0.2677272601865222\n",
            "mile kilometer 0.692 0.8745525551411975\n",
            "computer news 0.40800000000000003 0.5312225653455442\n",
            "atmosphere landscape 0.358 0.5223236050248317\n",
            "president medal 0.33799999999999997 0.313330711052532\n",
            "skin eye 0.46900000000000003 0.7413427761441903\n",
            "theater history 0.265 0.3760109815973836\n",
            "volunteer motto 0.185 0.37814376890996454\n",
            "prejudice recognition 0.273 0.39229328424226406\n",
            "century year 0.654 0.5022135388994471\n",
            "century nation 0.146 0.5405671041517961\n",
            "delay racism 0.10800000000000001 0.1750329239266056\n",
            "delay news 0.188 0.4506426448046934\n",
            "peace plan 0.369 0.6760286505711345\n",
            "minority peace 0.28500000000000003 0.4516324655037249\n",
            "attempt peace 0.23500000000000001 0.6071023091380889\n",
            "deployment departure 0.7809999999999999 0.5704774497809102\n",
            "announcement news 0.762 0.6865998359745286\n",
            "announcement effort 0.308 0.5149448731883954\n",
            "journal association 0.3 0.5716129009136588\n",
            "doctor personnel 0.5730000000000001 0.34172158980239514\n",
            "school center 0.508 0.6792837210971778\n",
            "reason hypertension 0.06899999999999999 0.1854014379269617\n",
            "hospital infrastructure 0.542 0.32606130980309117\n",
            "life death 0.696 0.7264110537105963\n",
            "life term 0.5349999999999999 0.6573973574266014\n",
            "word similarity 0.2 0.49793796255011136\n",
            "board recommendation 0.25 0.7303833537697982\n",
            "governor interview 0.131 0.45812912847492476\n",
            "peace atmosphere 0.231 0.45619782524827385\n",
            "peace insurance 0.06899999999999999 0.1605642196036544\n",
            "travel activity 0.485 0.4702950655278984\n",
            "consumer confidence 0.40800000000000003 0.6529131324120888\n",
            "consumer energy 0.45 0.635296741397287\n",
            "problem airport 0.192 0.30604591679673204\n",
            "car flight 0.346 0.5361337883449632\n",
            "month hotel 0.031 0.46949243656964046\n",
            "type kind 0.931 0.6426395362878196\n",
            "situation conclusion 0.346 0.6922676156231776\n",
            "situation isolation 0.188 0.612350527609281\n",
            "direction combination 0.081 0.5301723603916592\n",
            "street place 0.608 0.5834720781559012\n",
            "street avenue 0.877 0.7968158839280524\n",
            "street block 0.7849999999999999 0.6042573058961251\n",
            "street children 0.154 0.35632032684380166\n",
            "listing proximity 0.196 0.3019450704396513\n",
            "cell phone 0.8539999999999999 0.6038961152058903\n",
            "production hike 0.1 0.38826485671412425\n",
            "benchmark index 0.446 0.8684665289748409\n",
            "media trading 0.11499999999999999 0.5055172042503311\n",
            "media gain 0.131 0.5090456484451621\n",
            "dividend payment 0.8380000000000001 0.6891621492044198\n",
            "calculation computation 0.9380000000000001 0.7398957585253295\n",
            "announcement production 0.238 0.432035844881368\n",
            "profit warning 0.077 0.39299315601146667\n",
            "profit loss 0.669 0.6723989212929273\n",
            "dollar yen 0.808 0.7447424489953054\n",
            "dollar buck 0.954 0.20557434565922286\n",
            "phone equipment 0.6849999999999999 0.5856219945419394\n",
            "five month 0.10800000000000001 0.7635736747427656\n",
            "report gain 0.3 0.44683981612754914\n",
            "liquid water 0.8619999999999999 0.7200360916107071\n",
            "marathon sprint 0.5690000000000001 0.6737567541730106\n",
            "seven series 0.238 0.6888040095703242\n",
            "seafood food 0.852 0.7335304231017403\n",
            "seafood lobster 0.86 0.7446100067277948\n",
            "lobster food 0.837 0.41027995583667315\n",
            "lobster wine 0.254 0.4738479091253358\n",
            "start year 0.142 0.7834184270992299\n",
            "start match 0.20800000000000002 0.6903815319192137\n",
            "championship tournament 0.792 0.8834082109098912\n",
            "line insurance 0.11499999999999999 0.3934904302932564\n",
            "man woman 0.8380000000000001 0.886033771849582\n",
            "man governor 0.6 0.40517966188855664\n",
            "murder manslaughter 0.765 0.7269791744716312\n",
            "opera performance 0.746 0.5566371546367646\n",
            "focus life 0.446 0.679615167368834\n",
            "viewer serial 0.304 0.33561472403995524\n",
            "possibility girl 0.246 0.2988172192596219\n",
            "population development 0.442 0.4886047831540054\n",
            "morality importance 0.446 0.5176231324307184\n",
            "morality marriage 0.477 0.48939339798708215\n",
            "opera industry 0.33799999999999997 0.3271008701954398\n",
            "sugar approach 0.1 0.13660301703096464\n",
            "practice institution 0.331 0.49758608349862143\n",
            "ministry culture 0.43499999999999994 0.46421661394057445\n",
            "development issue 0.458 0.5379986653560404\n",
            "experience music 0.454 0.558559975299236\n",
            "music project 0.446 0.5280009852343869\n",
            "glass metal 0.458 0.7708716139798338\n",
            "aluminum metal 0.762 0.7328648284701462\n",
            "chance credibility 0.41200000000000003 0.5396703095995518\n",
            "rock jazz 0.6849999999999999 0.7286166885245555\n",
            "museum theater 0.6 0.6515203004753635\n",
            "observation architecture 0.362 0.3953504437805353\n",
            "shower thunderstorm 0.673 0.45250803923863137\n",
            "architecture century 0.331 0.7023030782933987\n"
          ]
        }
      ],
      "source": [
        "sim_table = []\n",
        "\n",
        "with open(\"word_sim.txt\") as f: \n",
        "    next(f) # skip header\n",
        "    for line in f: \n",
        "        try: \n",
        "            line = line.split()\n",
        "            word1_emb = w2i[line[0]]\n",
        "            word2_emb = w2i[line[1]]\n",
        "            # print(word1_emb, word2_emb)\n",
        "            dist = cosine_distance(wvecs[word1_emb] , wvecs[word2_emb])\n",
        "            human_score = float(line[2])/10\n",
        "            print(line[0], line[1], human_score, dist)\n",
        "            sim_table.append([line[0], line[1], human_score, dist])\n",
        "            \n",
        "        except: \n",
        "            pass\n",
        "        # print(line, dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.4246259666521494, 6.801092849706633e-10)"
            ]
          },
          "execution_count": 292,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "sim_table = np.array(sim_table)\n",
        "x = np.double(sim_table[:, 2])\n",
        "y = np.double(sim_table[:, 3])\n",
        "\n",
        "pearsonr(x, y) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Kev8-wA1QA"
      },
      "source": [
        "##  Exploring Analogies\n",
        "\n",
        "The second task we consider **completing analogies**. We are given an incomplete analogy of the form: \n",
        "\n",
        "\n",
        "* $a : b : : c :~?$\n",
        "\n",
        "\n",
        "We would then identify the word vector which maximizes the cosine similarity. \n",
        "This metric has an intuitive interpretation. Ideally, we want $\\phi(b) - \\phi(a) = \\phi(d) - \\phi(c)$ where $\\phi(.)$ is the word vector. \n",
        "For instance, \n",
        "\n",
        "* *london $-$ england = paris $-$ france* .\n",
        "\n",
        "Thus we identify the vector $\\phi(d)$ which maximizes the normalized dot-product between the two word\n",
        "vectors (i.e. cosine similarity).\n",
        "\n",
        "\n",
        "\n",
        "* You can either use your own method to compute the correct word or use the code below. \n",
        "\n",
        "* Use original analogies dataset https://github.com/svn2github/word2vec/blob/master/questions-words.txt \n",
        "\n",
        "Q. When does it fail? \n",
        "\n",
        "The vector space does not capture all semantic relationships between words. For example, if I were to do happy and sad, and bad and __, this will fail. \n",
        "\n",
        "Q. What are the possible reasons for failure?\n",
        "\n",
        "##### 15mins\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "xWVu38ePjh-i"
      },
      "outputs": [],
      "source": [
        "def find_analogy(word_a, word_b, word_c, word_vectors, word2index):\n",
        "    word_a = word_a.lower()\n",
        "    word_b = word_b.lower()\n",
        "    word_c = word_c.lower()\n",
        "    \n",
        "    (e_a, e_b, e_c) = (word_vectors[word2index[word_a]], \n",
        "                       word_vectors[word2index[word_b]], \n",
        "                       word_vectors[word2index[word_c]])\n",
        "    \n",
        "    \n",
        "    max_cosine_sim = -999\n",
        "    best_word = None\n",
        "    \n",
        "    for (w, i) in word2index.items():\n",
        "        if w in [word_a, word_b, word_c]:\n",
        "            continue\n",
        "        cosine_sim = cosine_distance(e_b - e_a, word_vectors[i] - e_c)\n",
        "        \n",
        "        if cosine_sim > max_cosine_sim:\n",
        "            max_cosine_sim = cosine_sim\n",
        "            best_word = w\n",
        "            \n",
        "    return best_word\n",
        "  \n",
        "# find_analogy('france', 'paris', 'england', wvecs, w2i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'london'"
            ]
          },
          "execution_count": 294,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_analogy('france', 'paris', 'england', wvecs, w2i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'irony'"
            ]
          },
          "execution_count": 296,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_analogy('happy', 'sad', 'good', wvecs, w2i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'agricultural'"
            ]
          },
          "execution_count": 297,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_analogy(\"cat\", \"animal\", \"rose\", wvecs, w2i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqchbyIiCOcQ"
      },
      "source": [
        "# Advanced: Compositionality \n",
        "\n",
        "* Given access to only word representations, how can we build representations for phrases and sentences? \n",
        "\n",
        "  (Hint: algebraic operation is one way) \n",
        "\n",
        "\n",
        "* Compute the similarity score between two sentences on the STS.input.MSRpar.txt dataset from https://github.com/alvations/stasis/tree/master/STS-data/STS2012-train \n",
        "\n",
        "  (Please use 00-readme.txt in the corpus for details on the format)\n",
        "  \n",
        "* Measure the pearson correlation with the human scores in STS.gs.MSRpar.txt\n",
        "\n",
        "Q. What problems did you encounter when computing the scores? \n",
        "\n",
        "Q. What are alternative ways of computing the scores? \n",
        "\n",
        "Q. Using your composition method, compute representations for the following expressions and also list the top-5 most similar words: \n",
        "\n",
        "* New York \n",
        "* kick the bucket\n",
        "* post office\n",
        "\n",
        "  Does it work? What are the possible reasons? \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUwxb7CPt_n3"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "* [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf): word2vec reference\n",
        "\n",
        "* [Eluciating the properties of semantic word representations](http://www.offconvex.org/2016/02/14/word-embeddings-1/): A global perspective\n",
        "\n",
        "* [Understanding the algebraic notions of semantic word representations](http://www.offconvex.org/2015/12/12/word-embeddings-1/): Why does the word-analogies task work with simple algebraic manipulations?\n",
        "\n",
        "* [Stemming And Lemmatization](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "01_preprocessing_and_embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
